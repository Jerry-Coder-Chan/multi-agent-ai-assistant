# -*- coding: utf-8 -*-
"""rag_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OLdo8OZFnS51uka7mrHCGZ8VQafhsbPG
"""

# ============================================================================
# FILE: agents/rag_agent.py
# ============================================================================
"""
RAG Agent - Document-based Question Answering.
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import Optional


class RAGAgent:
    """Handles document-based question answering."""

    def __init__(self, api_key: str, pdf_path: str, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        self.vector_store = None
        self.chain = None

        # Load and process document
        self._load_document(pdf_path)

    def _load_document(self, pdf_path: str):
        """Load PDF and create vector store."""
        # Load PDF
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()

        # Split into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)

        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)
        self.vector_store = FAISS.from_documents(chunks, embeddings)

        # Create retrieval chain
        self._create_chain()

    def _create_chain(self):
        """Create the RAG chain."""
        llm = ChatOpenAI(
            model=self.model,
            temperature=0,
            openai_api_key=self.api_key
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful assistant answering questions about events.
Use the provided context to answer accurately. If you don't know, say so."""),
            ("user", "Context: {context}\n\nQuestion: {question}")
        ])

        retriever = self.vector_store.as_retriever(search_kwargs={"k": 4})

        self.chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

    def query(self, question: str) -> str:
        """Answer a question using RAG."""
        try:
            return self.chain.invoke(question)
        except Exception as e:
            raise Exception(f"RAG query error: {str(e)}")